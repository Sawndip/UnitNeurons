---
layout: docs_default
title: Unit Neurons Documentation - Gradient Descent
---

<h1>Gradient Descent</h1>

  <h2>Float Feedforward Neuron</h2>

    <h3>Constructors</h3>
      <p><code>FloatFeedForwardNeuron(_prevs, _num_prevs, _query_manager, &_activ)</code></p>
      <ul>
        <li>
          <code>_prevs</code>(<code>FloatUnitNeuron**</code>)
          <ul>
            Array of memory addresses pointing to <code>FloatUnitNeuron</code>
            and its subclass instances
          </ul>
          <ul>
            Indicates <code>previous</code> neurons that the current neuron derives
            <code>state</code> data from
          </ul>
          <ul>
            If the assigned array consists of pointers to subclasses of
            <code>FloatUnitNeuron</code>, include <code>(FloatUnitNeuron**)</code>
            in front of the variable
          </ul>
        </li>
        <li>
          <code>_num_prevs</code>
          <ul>
            Number of elements (<code>int</code>) in the <code>_prevs</code> array
          </ul>
        </li>
        <li>
          <code>_query_manager</code>(<code>FeedbackQueryManager*</code>)
          <ul>
            Reference to the <code>FeedbackQueryManager</code> instance
          </ul>
        </li>
        <li>
          <code>&_activ</code>(<code>std::string const</code>)
          <ul>
            String literal of either of the following indicating the
            activation function of the neuron:
            <li><code>"identity"</code></li>
            <li><code>"relu"</code></li>
            <li><code>"tanh"</code></li>
            <li><code>"sigmoid"</code></li>
          </ul>
          <ul>
            For neurons with softmax function as activation function, set
            the activation function as <code>"identity"</code> and use an
            external softmax function
            <code>float* softmax (float* x, int size)</code>
            located at the
            <a href="https://github.com/johnlime/UnitNeurons/blob/master/Unit%20Neurons/gradient_descent/gradient_descent.hpp#L56">
              header file
            </a>
          </ul>
        </li>
      </ul>

      <p><code>FloatFeedForwardNeuron(_prevs, _num_prevs, _query_manager, float (*_activation) (float), float (*_gradient) (float))</code></p>
      <ul>
        <li><code>_prevs</code>
          <ul>
            Array of memory addresses pointing to <code>FloatUnitNeuron</code>
            and its subclass instances
          </ul>
        </li>
        <li><code>_num_prevs</code>
          <ul>
            Number of elements (<code>int</code>) in <code>_prevs</code>
          </ul>
        </li>
        <li><code>_query_manager</code>
          <ul>
            Reference to the <code>FeedbackQueryManager</code> instance
          </ul>
        </li>
        <li><code>float (*_activation) (float)</code>
          <ul>
            Unnamed activation function
          </ul>
          <ul>
            Function pointer that depicts the activation function
          </ul>
        </li>
        <li><code>float (*_gradient) (float)</code>
          <ul>
            Unnamed derivative of <code>_activation</code>
          </ul>
          <ul>
            Function pointer that depicts the derivative of <code>_activation</code>
          </ul>
          <ul>
            The input <code>float</code> is synonymous to that of the activation function
          </ul>
        </li>
      </ul>

    <h3>Structure</h3>
      <h4>Public Methods and Variables</h4>
        <ul>
          <li><code>lr</code>(floating point value)
            <ul>Learning rate for weight update</ul>
            <ul>The default value is set to <code>0.7f</code></ul>
          </li>
          <li><code>void feedforward()</code>
            <ul>
              Weight values stored in <code>memory</code> are multiplied with
              <code>state</code> values from <code>previous</code> neurons
              and put through the activation function defined during construction.
            </ul>
          </li>
          <li><code>void feedback(float* fb_input)</code>
            <ul>
              Performs gradient descent operation on the specific neuron and
              produces instances of <code>FeedbackQuery</code> which are added to
              the internal array in the assigned <code>query_manager</code>
            </ul>
            <ul>
              Gradients of each neuron are calculated analytically. Have a look
              at the
              <a href="https://github.com/johnlime/UnitNeurons/blob/master/Unit%20Neurons/gradient_descent/float_gd_ff_neuron.cpp#L116">
                code
              </a>
              for details.
            </ul>
            <ul>
              For more descriptive derivation, look at "Last Layer" and
              "Hidden Layers" sections of the
              <a href="https://towardsdatascience.com/part-2-gradient-descent-and-backpropagation-bf90932c066a">
                "Gradient Descent and Back Propagation" article by Tobias Hill in the Medium
              </a>.
            </ul>
          </li>
        </ul>

      <h4>Additional Notes and Code</h4>
        <p>
          For more details on private variables, methods, and other functionalities
          performed within the definition of the class, refer to the
          <a href="https://github.com/johnlime/UnitNeurons/blob/master/Unit%20Neurons/gradient_descent/gradient_descent.hpp#L19">
            hpp file
          </a>
          and the
          <a href="https://github.com/johnlime/UnitNeurons/blob/master/Unit%20Neurons/gradient_descent/float_gd_ff_neuron.cpp">
            cpp file
          </a>.
        </p>

<h2>Float Gradient Descent</h2>
  <p>
    Subclass of <code>GlobalOperator</code> that starts the gradient descent
    training process of the neural network
  </p>

  <h3>Constructors</h3>
    <code>FloatGradientDescent(_targets, _num_targets)</code>
    <ul>
      <li><code>_targets</code>
        <ul>
          Array of memory addresses to <code>FloatFeedForwardNeuron</code>
          used as the output neurons
        </ul>
      </li>
      <li><code>_num_targets</code>
        <ul>
          Number of elements (<code>int</code>) in <code>_targets</code>
        </ul>
      </li>
    </ul>

  <h3>Training</h3>
    <p>
      <code>FloatGradientDescent</code> contains functions that executes the
      <code>feedback(float* fb_input)</code> of the output neurons.
    </p>
    <p>
      However, before executing <code>execute()</code>, the global operator must
      calculate the loss function given a correct output (refer to supervised
      learning).
    </p>

    <h4>Minimum Square Loss</h4>
    <p>
      Full list of tools for MSL calculation can be found
      <a href="https://github.com/johnlime/UnitNeurons/blob/master/Unit%20Neurons/gradient_descent/gradient_descent.hpp#L43">
        here
      </a>.
    </p>
    <p><code>void calculate_l1_loss(int* indices, int length, float* correct_value, float* coef)</code></p>
    <ul>
      <li><code>indices</code>
        <ul>
          Indices of specific neurons to start feedback loop from based on
          the internal <code>targets</code> array
        </ul>
      </li>
      <li><code>length</code>
        <ul>
          Number of elements in <code>indices</code>
        </ul>
      </li>
      <li><code>correct_value</code>
        <ul>
          Array of correct value to calculate MSL from
        </ul>
      </li>
      <li><code>coef</code>
        <ul>
          Coefficient value to multiply the calculated gradients with
        </ul>
      </li>
    </ul>

    <h4>Cross Entropy Loss</h4>
    <p>
      Full list of tools for CEL calculation can be found
      <a href="https://github.com/johnlime/UnitNeurons/blob/master/Unit%20Neurons/gradient_descent/gradient_descent.hpp#L48">
        here
      </a>.
    </p>
    <p><code>void calculate_cross_entropy_loss(float* correct_value, float* coef)</code></p>
    <ul>
      <li><code>correct_value</code>
        <ul>
          Array of correct value to calculate CEL from
        </ul>
      </li>
      <li><code>coef</code>
        <ul>
          Coefficient value to multiply the calculated gradients with
        </ul>
      </li>
    </ul>
    <p><code>void calculate_cross_entropy_loss(int index, float coef)</code></p>
    <ul>
      <li><code>index</code>
        <ul>
          The correct index (used in classification problems)
        </ul>
      </li>
      <li><code>coef</code>
        <ul>
          Coefficient value to multiply the calculated gradients with
        </ul>
      </li>
    </ul>

  <h3>Code</h3>
    <p>
      For more details on private variables, methods, and other functionalities
      performed within the definition of the class, refer to the
      <a href="https://github.com/johnlime/UnitNeurons/blob/master/Unit%20Neurons/gradient_descent/gradient_descent.hpp">
        hpp file
      </a>
      and the
      <a href="https://github.com/johnlime/UnitNeurons/blob/master/Unit%20Neurons/gradient_descent/gradient_descent.cpp">
        cpp file
      </a>.
    </p>
